{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#init\n",
    "import torch, numpy, sys\n",
    "sys.path.append(\"..\")\n",
    "from chatbot_end2end.module.interact_session import InteractSession\n",
    "from nlptools.utils import Config\n",
    "import torch.nn as nn\n",
    "\n",
    "cfg = Config(\"../config/elsa.yml\")\n",
    "session = InteractSession.build(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n",
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "skill_name = \"cornell\"\n",
    "skill = session.topic_manager.skills[skill_name]\n",
    "state_dict = skill.model.decoder.state_dict()\n",
    "dialog_status = session.new_dialog()\n",
    "dialog_status.add_utterance(\"I would hardly say you look like trash\")\n",
    "data = dialog_status.status2data()\n",
    "data.to(\"cuda:0\")\n",
    "\n",
    "encoder_out = skill.model.dialog_embedding(data[\"utterance\"].data, data[\"utterance_mask\"].data, data[\"sentiment\"].data)\n",
    "utterance_mask = data[\"utterance_mask\"].data\n",
    "print(encoder_out.shape)\n",
    "print(utterance_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-22.7928, -22.5807, -22.7010,  ..., -21.8003, -22.2742, -23.3795],\n",
      "        [-22.7928, -22.5807, -22.7010,  ..., -21.8003, -22.2742, -23.3795],\n",
      "        [-22.7928, -22.5807, -22.7010,  ..., -21.8003, -22.2742, -23.3795],\n",
      "        [-22.7928, -22.5807, -22.7010,  ..., -21.8003, -22.2742, -23.3795],\n",
      "        [-22.7928, -22.5807, -22.7010,  ..., -21.8003, -22.2742, -23.3795]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([0., 0., 0., 0., 0.], device='cuda:0') torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897],\n",
      "        [-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897],\n",
      "        [-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897],\n",
      "        [-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897],\n",
      "        [-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-11.3964, -11.2903, -11.3505,  ..., -10.9001, -11.1371, -11.6897]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) torch.Size([1, 30522])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[ -9.8358,  -9.8454,  -9.8548,  -9.8556, -10.0402]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[29690, 15223, 28650,   102, 20117]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650,   102, 20117], device='cuda:0')\n",
      "output_max_prev tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([ -9.8358,  -9.8454,  -9.8548,  -9.8556, -10.0402], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.2758, -20.2305, -20.3738,  ..., -21.4729, -21.6354, -19.9957],\n",
      "        [-27.0157, -26.9939, -27.1411,  ..., -28.8076, -28.3275, -25.7798],\n",
      "        [-24.4741, -24.1967, -24.2438,  ..., -25.7795, -26.1086, -23.6163],\n",
      "        [-10.5836, -10.4027, -10.6266,  ..., -10.1146,  -9.8498, -10.2527],\n",
      "        [-21.1547, -21.1407, -21.2531,  ..., -22.8377, -22.5070, -22.4763]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([ -9.8358,  -9.8454,  -9.8548,  -9.8556, -10.0402], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-15.0558, -15.0331, -15.1048,  ..., -15.6543, -15.7356, -14.9157],\n",
      "        [-18.4305, -18.4197, -18.4933,  ..., -19.3265, -19.0865, -17.8126],\n",
      "        [-17.1644, -17.0258, -17.0493,  ..., -17.8171, -17.9817, -16.7355],\n",
      "        [-10.2196, -10.1291, -10.2411,  ...,  -9.9851,  -9.8527, -10.0541],\n",
      "        [-15.5975, -15.5905, -15.6466,  ..., -16.4390, -16.2736, -16.2582]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-15.0558, -15.0331, -15.1048,  ..., -16.4390, -16.2736, -16.2582]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-4.9179, -4.9227, -4.9274, -5.0201, -9.8556]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 142205,  91566]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117,     0], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 4, 3], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-4.9179, -4.9227, -4.9274, -5.0201, -9.8556], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.5319, -20.5473, -20.5931,  ..., -21.5763, -21.9779, -20.1253],\n",
      "        [-27.5628, -27.6093, -27.6662,  ..., -29.2619, -29.0008, -26.1365],\n",
      "        [-25.0594, -24.8451, -24.7909,  ..., -26.2771, -26.8285, -24.0879],\n",
      "        [-21.5400, -21.5791, -21.6010,  ..., -23.1515, -22.9592, -22.8942],\n",
      "        [ -6.9665,  -7.2863,  -7.2553,  ...,  -9.7250,  -9.9562, -10.2190]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-4.9179, -4.9227, -4.9274, -5.0201, -9.8556], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-12.7249, -12.7326, -12.7555,  ..., -13.2471, -13.4479, -12.5216],\n",
      "        [-16.2427, -16.2660, -16.2945,  ..., -17.0923, -16.9618, -15.5296],\n",
      "        [-14.9934, -14.8862, -14.8591,  ..., -15.6022, -15.8779, -14.5077],\n",
      "        [-13.2801, -13.2996, -13.3105,  ..., -14.0858, -13.9897, -13.9572],\n",
      "        [ -8.4110,  -8.5709,  -8.5555,  ...,  -9.7903,  -9.9059, -10.0373]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-1.2725e+01, -1.2733e+01, -1.2756e+01,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-2.4590, -2.4614, -2.4637, -2.5101, -9.5802]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 103824]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 3], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-2.4590, -2.4614, -2.4637, -2.5101, -9.5802], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.7469, -20.7661, -20.8193,  ..., -21.8118, -22.0956, -20.1684],\n",
      "        [-27.8419, -27.8921, -27.9638,  ..., -29.5910, -29.1891, -26.2614],\n",
      "        [-25.3784, -25.1707, -25.1144,  ..., -26.6453, -27.0615, -24.2973],\n",
      "        [-21.8470, -21.8872, -21.9247,  ..., -23.5254, -23.1754, -23.1612],\n",
      "        [-23.1231, -23.1176, -23.0254,  ..., -24.7151, -24.1421, -23.1415]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-2.4590, -2.4614, -2.4637, -2.5101, -9.5802], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-11.6029, -11.6125, -11.6391,  ..., -12.1354, -12.2773, -11.3137],\n",
      "        [-15.1516, -15.1767, -15.2126,  ..., -16.0262, -15.8252, -14.3614],\n",
      "        [-13.9210, -13.8172, -13.7890,  ..., -14.5545, -14.7626, -13.3805],\n",
      "        [-12.1785, -12.1986, -12.2174,  ..., -13.0177, -12.8427, -12.8357],\n",
      "        [-16.3517, -16.3489, -16.3028,  ..., -17.1476, -16.8611, -16.3609]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-11.6029, -11.6125, -11.6391,  ..., -17.1476, -16.8611, -16.3609]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-1.2295, -1.2307, -1.2318, -1.2550, -4.7901]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-1.2295, -1.2307, -1.2318, -1.2550, -4.7901], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.6859, -20.7277, -20.7991,  ..., -21.7736, -22.0259, -20.1782],\n",
      "        [-27.6557, -27.7260, -27.8228,  ..., -29.4369, -29.0364, -26.1408],\n",
      "        [-25.3747, -25.1872, -25.1427,  ..., -26.7019, -27.1039, -24.3849],\n",
      "        [-21.9361, -21.9922, -22.0536,  ..., -23.6535, -23.2570, -23.3535],\n",
      "        [-22.9658, -22.9764, -22.9027,  ..., -24.6145, -23.9924, -23.0720]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-1.2295, -1.2307, -1.2318, -1.2550, -4.7901], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.9577, -10.9786, -11.0143,  ..., -11.5015, -11.6277, -10.7039],\n",
      "        [-14.4432, -14.4783, -14.5267,  ..., -15.3338, -15.1336, -13.6857],\n",
      "        [-13.3033, -13.2095, -13.1873,  ..., -13.9669, -14.1679, -12.8084],\n",
      "        [-11.5955, -11.6236, -11.6543,  ..., -12.4543, -12.2560, -12.3043],\n",
      "        [-13.8779, -13.8832, -13.8464,  ..., -14.7023, -14.3912, -13.9310]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.9577, -10.9786, -11.0143,  ..., -14.7023, -14.3912, -13.9310]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.6147, -0.6153, -0.6159, -0.6275, -2.3950]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.6147, -0.6153, -0.6159, -0.6275, -2.3950], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.5090, -20.5531, -20.6117,  ..., -21.5332, -21.9803, -20.1619],\n",
      "        [-27.6052, -27.6819, -27.7649,  ..., -29.3714, -29.1346, -26.2467],\n",
      "        [-25.1724, -24.9868, -24.9304,  ..., -26.4663, -27.0577, -24.3415],\n",
      "        [-21.8293, -21.8909, -21.9349,  ..., -23.4971, -23.2875, -23.4396],\n",
      "        [-22.6688, -22.6757, -22.5875,  ..., -24.2714, -23.8363, -22.9478]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.6147, -0.6153, -0.6159, -0.6275, -2.3950], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.5619, -10.5839, -10.6132,  ..., -11.0740, -11.2975, -10.3883],\n",
      "        [-14.1103, -14.1486, -14.1901,  ..., -14.9934, -14.8750, -13.4310],\n",
      "        [-12.8942, -12.8013, -12.7731,  ..., -13.5411, -13.8368, -12.4787],\n",
      "        [-11.2284, -11.2592, -11.2812,  ..., -12.0623, -11.9575, -12.0336],\n",
      "        [-12.5319, -12.5354, -12.4912,  ..., -13.3332, -13.1156, -12.6714]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.5619, -10.5839, -10.6132,  ..., -13.3332, -13.1156, -12.6714]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.3074, -0.3077, -0.3080, -0.3138, -1.1975]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690, 29690,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223, 15223,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.3074, -0.3077, -0.3080, -0.3138, -1.1975], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.5358, -20.5900, -20.6573,  ..., -21.5404, -21.9860, -20.1338],\n",
      "        [-27.6955, -27.7793, -27.8776,  ..., -29.4535, -29.2128, -26.3022],\n",
      "        [-25.2630, -25.0860, -25.0325,  ..., -26.5620, -27.1206, -24.3645],\n",
      "        [-21.9021, -21.9653, -22.0224,  ..., -23.5590, -23.3231, -23.4886],\n",
      "        [-22.7913, -22.7987, -22.7252,  ..., -24.3915, -23.9420, -23.0399]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.3074, -0.3077, -0.3080, -0.3138, -1.1975], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.4216, -10.4487, -10.4823,  ..., -10.9239, -11.1467, -10.2206],\n",
      "        [-14.0016, -14.0435, -14.0926,  ..., -14.8806, -14.7602, -13.3049],\n",
      "        [-12.7855, -12.6970, -12.6702,  ..., -13.4350, -13.7143, -12.3362],\n",
      "        [-11.1080, -11.1395, -11.1681,  ..., -11.9364, -11.8184, -11.9012],\n",
      "        [-11.9944, -11.9981, -11.9614,  ..., -12.7945, -12.5698, -12.1187]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.4216, -10.4487, -10.4823,  ..., -12.7945, -12.5698, -12.1187]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.1537, -0.1538, -0.1540, -0.1569, -0.5988]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.1537, -0.1538, -0.1540, -0.1569, -0.5988], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.6005, -20.6503, -20.7651,  ..., -21.5191, -22.1920, -20.0493],\n",
      "        [-27.7772, -27.8603, -28.0062,  ..., -29.4458, -29.4481, -26.2396],\n",
      "        [-25.3869, -25.2097, -25.2011,  ..., -26.6184, -27.3989, -24.3712],\n",
      "        [-21.8390, -21.8994, -22.0017,  ..., -23.4279, -23.3922, -23.3156],\n",
      "        [-22.7628, -22.7712, -22.7441,  ..., -24.2892, -24.0493, -22.8678]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.1537, -0.1538, -0.1540, -0.1569, -0.5988], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.3771, -10.4020, -10.4594,  ..., -10.8364, -11.1729, -10.1015],\n",
      "        [-13.9655, -14.0071, -14.0800,  ..., -14.7998, -14.8010, -13.1967],\n",
      "        [-12.7704, -12.6818, -12.6775,  ..., -13.3862, -13.7764, -12.2626],\n",
      "        [-10.9980, -11.0281, -11.0793,  ..., -11.7924, -11.7745, -11.7362],\n",
      "        [-11.6808, -11.6850, -11.6714,  ..., -12.4440, -12.3240, -11.7333]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.3771, -10.4020, -10.4594,  ..., -12.4440, -12.3240, -11.7333]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0769, -0.0769, -0.0770, -0.0784, -0.2994]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0769, -0.0769, -0.0770, -0.0784, -0.2994], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.3763, -20.3979, -20.5238,  ..., -21.2464, -21.8184, -19.7694],\n",
      "        [-27.6559, -27.7138, -27.8651,  ..., -29.2956, -29.1885, -26.0556],\n",
      "        [-25.2136, -25.0104, -25.0139,  ..., -26.3966, -27.0789, -24.1364],\n",
      "        [-21.6539, -21.6929, -21.7999,  ..., -23.2071, -23.0715, -23.0818],\n",
      "        [-22.4346, -22.4147, -22.4007,  ..., -23.8958, -23.5658, -22.4532]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0769, -0.0769, -0.0770, -0.0784, -0.2994], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2266, -10.2374, -10.3003,  ..., -10.6616, -10.9476,  -9.9232],\n",
      "        [-13.8664, -13.8954, -13.9710,  ..., -14.6862, -14.6327, -13.0662],\n",
      "        [-12.6453, -12.5437, -12.5455,  ..., -13.2368, -13.5779, -12.1067],\n",
      "        [-10.8662, -10.8856, -10.9392,  ..., -11.6428, -11.5750, -11.5801],\n",
      "        [-11.3670, -11.3570, -11.3500,  ..., -12.0976, -11.9326, -11.3763]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2266, -10.2374, -10.3003,  ..., -12.0976, -11.9326, -11.3763]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0384, -0.0385, -0.0385, -0.0392, -0.1497]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0384, -0.0385, -0.0385, -0.0392, -0.1497], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.3924, -20.4017, -20.5693,  ..., -21.2790, -21.6842, -19.6808],\n",
      "        [-27.5939, -27.6348, -27.8344,  ..., -29.2155, -28.9893, -25.8915],\n",
      "        [-25.2711, -25.0525, -25.0954,  ..., -26.4635, -26.9927, -24.0859],\n",
      "        [-21.8565, -21.8768, -22.0340,  ..., -23.4025, -23.1237, -23.1941],\n",
      "        [-22.5049, -22.4639, -22.4983,  ..., -23.9845, -23.4993, -22.4250]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0384, -0.0385, -0.0385, -0.0392, -0.1497], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2154, -10.2201, -10.3039,  ..., -10.6587, -10.8613,  -9.8596],\n",
      "        [-13.8162, -13.8366, -13.9364,  ..., -14.6270, -14.5139, -12.9650],\n",
      "        [-12.6548, -12.5455, -12.5669,  ..., -13.2510, -13.5156, -12.0622],\n",
      "        [-10.9479, -10.9580, -11.0366,  ..., -11.7209, -11.5815, -11.6167],\n",
      "        [-11.3273, -11.3068, -11.3240,  ..., -12.0671, -11.8245, -11.2874]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2154, -10.2201, -10.3039,  ..., -12.0671, -11.8245, -11.2874]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0192, -0.0192, -0.0192, -0.0196, -0.0748]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 29690,  45745,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([29690, 15223, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0192, -0.0192, -0.0192, -0.0196, -0.0748], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-20.3934, -20.3992, -20.5606,  ..., -21.2240, -21.7637, -19.8338],\n",
      "        [-27.4255, -27.4599, -27.6595,  ..., -28.9781, -28.9033, -25.8491],\n",
      "        [-25.1366, -24.9126, -24.9544,  ..., -26.2694, -26.9558, -24.1110],\n",
      "        [-21.8522, -21.8690, -22.0197,  ..., -23.3288, -23.2023, -23.3405],\n",
      "        [-22.4424, -22.3925, -22.4234,  ..., -23.8502, -23.5117, -22.4983]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0192, -0.0192, -0.0192, -0.0196, -0.0748], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2063, -10.2092, -10.2899,  ..., -10.6216, -10.8915,  -9.9265],\n",
      "        [-13.7223, -13.7396, -13.8394,  ..., -14.4986, -14.4612, -12.9342],\n",
      "        [-12.5779, -12.4659, -12.4868,  ..., -13.1443, -13.4875, -12.0651],\n",
      "        [-10.9359, -10.9443, -11.0197,  ..., -11.6742, -11.6110, -11.6801],\n",
      "        [-11.2586, -11.2337, -11.2491,  ..., -11.9625, -11.7933, -11.2866]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-10.2063, -10.2092, -10.2899,  ..., -11.9625, -11.7933, -11.2866]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0096, -0.0096, -0.0096, -0.0098, -0.0374]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 45745,  29690,  89694, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 29690, 28650, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([1, 0, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0096, -0.0096, -0.0096, -0.0098, -0.0374], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.5153, -27.5357, -27.7288,  ..., -29.1532, -28.9354, -25.9008],\n",
      "        [-20.4060, -20.3988, -20.5507,  ..., -21.3189, -21.7428, -19.8324],\n",
      "        [-25.1398, -24.9011, -24.9350,  ..., -26.3506, -26.9123, -24.0726],\n",
      "        [-21.9584, -21.9593, -22.1026,  ..., -23.5192, -23.2630, -23.4449],\n",
      "        [-22.5479, -22.4811, -22.5075,  ..., -24.0393, -23.5717, -22.6028]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0096, -0.0096, -0.0096, -0.0098, -0.0374], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.7625, -13.7726, -13.8692,  ..., -14.5814, -14.4725, -12.9552],\n",
      "        [-10.2078, -10.2042, -10.2802,  ..., -10.6643, -10.8762,  -9.9210],\n",
      "        [-12.5747, -12.4554, -12.4723,  ..., -13.1801, -13.4609, -12.0411],\n",
      "        [-10.9841, -10.9845, -11.0562,  ..., -11.7645, -11.6364, -11.7274],\n",
      "        [-11.2927, -11.2593, -11.2725,  ..., -12.0383, -11.8046, -11.3201]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.7625, -13.7726, -13.8692,  ..., -12.0383, -11.8046, -11.3201]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0048, -0.0048, -0.0048, -0.0049, -0.0187]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  89694,  60212, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 29690, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 2, 1, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0048, -0.0048, -0.0048, -0.0049, -0.0187], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.6710, -27.6966, -27.8987,  ..., -29.3778, -29.1945, -26.0681],\n",
      "        [-25.4181, -25.1910, -25.2224,  ..., -26.6945, -27.2872, -24.3741],\n",
      "        [-20.6474, -20.6483, -20.8034,  ..., -21.6371, -22.0957, -20.0838],\n",
      "        [-22.0245, -22.0276, -22.1793,  ..., -23.6689, -23.4198, -23.5343],\n",
      "        [-22.8140, -22.7596, -22.7882,  ..., -24.3962, -23.9278, -22.8948]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0048, -0.0048, -0.0048, -0.0049, -0.0187], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8379, -13.8507, -13.9517,  ..., -14.6913, -14.5997, -13.0364],\n",
      "        [-12.7115, -12.5979, -12.6136,  ..., -13.3497, -13.6460, -12.1895],\n",
      "        [-10.3261, -10.3265, -10.4041,  ..., -10.8210, -11.0503, -10.0443],\n",
      "        [-11.0147, -11.0163, -11.0921,  ..., -11.8369, -11.7124, -11.7696],\n",
      "        [-11.4163, -11.3892, -11.4035,  ..., -12.2074, -11.9732, -11.4568]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8379, -13.8507, -13.9517,  ..., -12.2074, -11.9732, -11.4568]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0024, -0.0024, -0.0024, -0.0025, -0.0094]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  90734, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 29690, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0024, -0.0024, -0.0024, -0.0025, -0.0094], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.7192, -27.7238, -27.9191,  ..., -29.4556, -29.2605, -26.0190],\n",
      "        [-25.4269, -25.1749, -25.2079,  ..., -26.7005, -27.3123, -24.3240],\n",
      "        [-20.5409, -20.5169, -20.6693,  ..., -21.5540, -22.0116, -19.9167],\n",
      "        [-21.9055, -21.8891, -22.0328,  ..., -23.5760, -23.3367, -23.3493],\n",
      "        [-22.5647, -22.4889, -22.5134,  ..., -24.1345, -23.6780, -22.5458]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0024, -0.0024, -0.0024, -0.0025, -0.0094], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8608, -13.8631, -13.9607,  ..., -14.7290, -14.6315, -13.0107],\n",
      "        [-12.7147, -12.5887, -12.6052,  ..., -13.3515, -13.6573, -12.1632],\n",
      "        [-10.2717, -10.2596, -10.3358,  ..., -10.7782, -11.0070,  -9.9596],\n",
      "        [-10.9540, -10.9458, -11.0176,  ..., -11.7892, -11.6696, -11.6759],\n",
      "        [-11.2870, -11.2491, -11.2614,  ..., -12.0719, -11.8437, -11.2776]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8608, -13.8631, -13.9607,  ..., -12.0719, -11.8437, -11.2776]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0012, -0.0012, -0.0012, -0.0012, -0.0047]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  90734, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 29690, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223,     0,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650,     0,     0,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117,     0,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0012, -0.0012, -0.0012, -0.0012, -0.0047], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.6151, -27.5967, -27.8088,  ..., -29.3481, -29.0066, -25.8657],\n",
      "        [-25.3486, -25.0777, -25.1210,  ..., -26.6156, -27.0690, -24.2098],\n",
      "        [-20.4499, -20.4101, -20.5732,  ..., -21.4461, -21.7491, -19.7740],\n",
      "        [-21.9142, -21.8795, -22.0410,  ..., -23.5719, -23.1814, -23.3182],\n",
      "        [-22.4750, -22.3784, -22.4190,  ..., -24.0264, -23.4336, -22.4371]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0012, -0.0012, -0.0012, -0.0012, -0.0047], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8082, -13.7990, -13.9050,  ..., -14.6746, -14.5039, -12.9334],\n",
      "        [-12.6749, -12.5395, -12.5611,  ..., -13.3084, -13.5351, -12.1055],\n",
      "        [-10.2256, -10.2057, -10.2872,  ..., -10.7236, -10.8752,  -9.8876],\n",
      "        [-10.9577, -10.9404, -11.0211,  ..., -11.7866, -11.5913, -11.6597],\n",
      "        [-11.2398, -11.1915, -11.2118,  ..., -12.0155, -11.7192, -11.2209]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8082, -13.7990, -13.9050,  ..., -12.0155, -11.7192, -11.2209]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0006, -0.0006, -0.0006, -0.0006, -0.0023]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  90734, 111683, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 29690, 20117, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223, 15223,     0,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650, 28650,     0,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690, 29690,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117, 20117,     0,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258, 12258,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0006, -0.0006, -0.0006, -0.0006, -0.0023], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.2531, -27.2341, -27.4678,  ..., -28.8573, -28.6854, -25.4939],\n",
      "        [-25.1671, -24.8962, -24.9593,  ..., -26.3510, -26.9286, -24.0325],\n",
      "        [-20.3548, -20.3155, -20.4959,  ..., -21.2434, -21.6599, -19.6596],\n",
      "        [-21.8097, -21.7731, -21.9538,  ..., -23.3542, -23.0860, -23.1569],\n",
      "        [-22.3695, -22.2666, -22.3264,  ..., -23.8059, -23.3469, -22.3040]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0006, -0.0006, -0.0006, -0.0006, -0.0023], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.6268, -13.6173, -13.7342,  ..., -14.4289, -14.3430, -12.7472],\n",
      "        [-12.5838, -12.4484, -12.4800,  ..., -13.1758, -13.4646, -12.0166],\n",
      "        [-10.1777, -10.1581, -10.2482,  ..., -10.6220, -10.8303,  -9.8301],\n",
      "        [-10.9052, -10.8868, -10.9772,  ..., -11.6774, -11.5433, -11.5788],\n",
      "        [-11.1859, -11.1344, -11.1643,  ..., -11.9041, -11.6746, -11.1532]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.6268, -13.6173, -13.7342,  ..., -11.9041, -11.6746, -11.1532]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0003, -0.0003, -0.0003, -0.0003, -0.0012]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172, 111683,  90734, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 20117, 29690, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 3, 2, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223, 15223, 15223,     0,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650, 28650, 28650,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117, 20117, 20117,     0,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690, 29690, 29690,     0,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258, 12258, 12258,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0003, -0.0003, -0.0003, -0.0003, -0.0012], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.5145, -27.4966, -27.6980,  ..., -29.0957, -28.9492, -25.8199],\n",
      "        [-25.3439, -25.0716, -25.1046,  ..., -26.4970, -27.1345, -24.2563],\n",
      "        [-21.9351, -21.9024, -22.0469,  ..., -23.4636, -23.2437, -23.3786],\n",
      "        [-20.4163, -20.3743, -20.5227,  ..., -21.2843, -21.7653, -19.7856],\n",
      "        [-22.4993, -22.3946, -22.4215,  ..., -23.9007, -23.4990, -22.5427]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0003, -0.0003, -0.0003, -0.0003, -0.0012], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.7574, -13.7484, -13.8491,  ..., -14.5480, -14.4747, -12.9101],\n",
      "        [-12.6721, -12.5359, -12.5524,  ..., -13.2486, -13.5674, -12.1283],\n",
      "        [-10.9677, -10.9513, -11.0236,  ..., -11.7319, -11.6220, -11.6894],\n",
      "        [-10.2083, -10.1873, -10.2615,  ..., -10.6423, -10.8828,  -9.8930],\n",
      "        [-11.2503, -11.1979, -11.2113,  ..., -11.9509, -11.7501, -11.2719]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.7574, -13.7484, -13.8491,  ..., -11.9509, -11.7501, -11.2719]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-0.0002, -0.0002, -0.0002, -0.0002, -0.0006]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  81161, 121256, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 20117, 29690, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,     0,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,     0,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,     0,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,     0,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258,     0,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-0.0002, -0.0002, -0.0002, -0.0002, -0.0006], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.6213, -27.6231, -27.8248,  ..., -29.3415, -29.1160, -25.8212],\n",
      "        [-25.4768, -25.2268, -25.2515,  ..., -26.7844, -27.3266, -24.2800],\n",
      "        [-22.1061, -22.0929, -22.2388,  ..., -23.7788, -23.4487, -23.4426],\n",
      "        [-20.6329, -20.6106, -20.7570,  ..., -21.6452, -22.0424, -19.8835],\n",
      "        [-22.8897, -22.8090, -22.8338,  ..., -24.4565, -23.9558, -22.8522]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-0.0002, -0.0002, -0.0002, -0.0002, -0.0006], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8107, -13.8116, -13.9125,  ..., -14.6708, -14.5581, -12.9107],\n",
      "        [-12.7385, -12.6135, -12.6258,  ..., -13.3923, -13.6634, -12.1401],\n",
      "        [-11.0531, -11.0465, -11.1195,  ..., -11.8895, -11.7244, -11.7214],\n",
      "        [-10.3165, -10.3054, -10.3786,  ..., -10.8227, -11.0213,  -9.9418],\n",
      "        [-11.4452, -11.4048, -11.4172,  ..., -12.2286, -11.9782, -11.4264]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8107, -13.8116, -13.9125,  ..., -12.2286, -11.9782, -11.4264]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-7.5115e-05, -7.5186e-05, -7.8508e-05, -8.7586e-05, -2.9656e-04]],\n",
      "       device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  81161, 121256, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 20117, 29690, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,     0],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,     0],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,     0],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,     0],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258,     0]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-7.5115e-05, -7.5186e-05, -7.8508e-05, -8.7586e-05, -2.9656e-04],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "====================\n",
      "output_buf torch.Size([5, 1])\n",
      "transformer_embedding torch.Size([5, 1, 768])\n",
      "encoder_padding_mask torch.Size([5, 1, 1, 20])\n",
      "decoder_layer_input torch.Size([5, 1, 768])\n",
      "self_attn torch.Size([5, 1, 768])\n",
      "encoder_attn torch.Size([5, 1, 768])\n",
      "output torch.Size([5, 1, 768])\n",
      "output_probs tensor([[-27.6576, -27.6903, -27.8844,  ..., -29.4285, -29.2672, -25.9052],\n",
      "        [-25.5635, -25.3383, -25.3584,  ..., -26.9042, -27.5423, -24.4416],\n",
      "        [-22.0365, -22.0507, -22.1883,  ..., -23.7649, -23.5057, -23.4400],\n",
      "        [-20.7584, -20.7627, -20.9030,  ..., -21.8147, -22.2886, -20.0685],\n",
      "        [-22.8303, -22.7804, -22.7956,  ..., -24.4553, -24.0141, -22.8079]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5, 30522])\n",
      "prev_scores tensor([-7.5115e-05, -7.5186e-05, -7.8508e-05, -8.7586e-05, -2.9656e-04],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([5]) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8288, -13.8452, -13.9422,  ..., -14.7143, -14.6337, -12.9526],\n",
      "        [-12.7818, -12.6692, -12.6792,  ..., -13.4521, -13.7712, -12.2208],\n",
      "        [-11.0183, -11.0254, -11.0942,  ..., -11.8825, -11.7529, -11.7200],\n",
      "        [-10.3792, -10.3814, -10.4515,  ..., -10.9074, -11.1444, -10.0343],\n",
      "        [-11.4153, -11.3904, -11.3979,  ..., -12.2278, -12.0072, -11.4041]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>) torch.Size([5, 30522])\n",
      "output_probs tensor([[-13.8288, -13.8452, -13.9422,  ..., -12.2278, -12.0072, -11.4041]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>) torch.Size([1, 152610])\n",
      "output_max torch.return_types.topk(\n",
      "values=tensor([[-3.7557e-05, -3.7593e-05, -4.0208e-05, -4.8561e-05, -1.5019e-04]],\n",
      "       device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 15223,  59172,  81161, 121256, 134346]], device='cuda:0'))\n",
      "output_max_current tensor([15223, 28650, 20117, 29690, 12258], device='cuda:0')\n",
      "output_max_prev tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "output_buf tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
      "         15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223],\n",
      "        [  101, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650,\n",
      "         28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650, 28650],\n",
      "        [  101, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117,\n",
      "         20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117, 20117],\n",
      "        [  101, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690,\n",
      "         29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690, 29690],\n",
      "        [  101, 20117, 20117, 12258, 12258, 12258, 12258, 12258, 12258, 12258,\n",
      "         12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258, 12258]],\n",
      "       device='cuda:0')\n",
      "scores_buf tensor([-3.7557e-05, -3.7593e-05, -4.0208e-05, -4.8561e-05, -1.5019e-04],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "scores_buf torch.Size([5])\n",
      "output_buf torch.Size([5, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223,\n",
       "          15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223, 15223]],\n",
       "        device='cuda:0'), array([-3.75572963e-05]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nlptools.zoo.encoders.transformer\n",
    "import chatbot_end2end.model.generative_tracker\n",
    "import importlib\n",
    "importlib.reload(nlptools.zoo.encoders.transformer)\n",
    "importlib.reload(chatbot_end2end.model.generative_tracker)\n",
    "\n",
    "from nlptools.zoo.encoders.transformer import TransformerDecoder\n",
    "from chatbot_end2end.model.generative_tracker import Generative_Tracker\n",
    "\n",
    "embedding = skill.model.encoder.embedding\n",
    "\n",
    "decoder_hidden_layers = skill.model.config[\"decoder_hidden_layers\"]\n",
    "decoder_attention_heads = skill.model.config[\"decoder_attention_heads\"]\n",
    "decoder_hidden_size = skill.model.config[\"decoder_hidden_size\"]\n",
    "bert_model_name = skill.model.config[\"bert_model_name\"]\n",
    "pad_id = skill.model.pad_id\n",
    "bos_id = skill.model.bos_id\n",
    "eos_id = skill.model.eos_id\n",
    "unk_id = skill.model.unk_id\n",
    "beam_size = 5\n",
    "len_penalty = 1\n",
    "unk_penalty = 1\n",
    "dropout=0\n",
    "\n",
    "shared_layers = {\"encoder\": skill.model.encoder}\n",
    "\n",
    "tracker = Generative_Tracker(shared_layers=shared_layers, skill_name=skill_name, bert_model_name=bert_model_name, decoder_hidden_laers=decoder_hidden_layers,\n",
    "                             decoder_attention_heads=decoder_attention_heads, decider_hidden_size=decoder_hidden_size, dropout=dropout, pad_id=pad_id, bos_id=bos_id,\n",
    "                            eos_id=eos_id, unk_id=unk_id, beam_size=beam_size, len_penalty=len_penalty, unk_penalty=unk_penalty)\n",
    "tracker = tracker.to(\"cuda:0\")\n",
    "\n",
    "decoder = TransformerDecoder(embedding, decoder_hidden_layers, decoder_attention_heads, decoder_hidden_size, dropout)\n",
    "decoder = decoder.to(\"cuda:0\")\n",
    "decoder.load_state_dict(state_dict)\n",
    "\n",
    "tracker.decoder = decoder\n",
    "\n",
    "tracker.beam_search(encoder_out, utterance_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
